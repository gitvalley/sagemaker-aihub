{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nsagemaker           1.60.2    \ntokenizers          0.7.0     \ntransformers        2.11.0    \n\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "!pip install -q sagemaker sagemaker[local]\n",
    "\n",
    "# Install `transformers` from master\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers|sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:pandas failed to import. Analytics features will be impaired or broken.\nWARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name airesearch to get Role path.\narn:aws:iam::294038372338:role/hunkimSagemaker\n"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/hunkim-transformer'\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='hunkimSagemaker')['Role']['Arn']\n",
    "    \n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "mkdir: EsperBERTo: File exists\n--2020-06-10 23:44:08--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\nResolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)...52.85.193.97, 52.85.193.51, 52.85.193.66, ...\nConnecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|52.85.193.97|:443... connected.\nHTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n\n    The file is already fully retrieved; nothing to do.\n\n"
    }
   ],
   "source": [
    "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
    "!mkdir EsperBERTo\n",
    "\n",
    "!wget -O EsperBERTo/oscar.eo.txt -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\ninput spec (in this case, just an S3 path): s3://sagemaker-us-west-2-294038372338/sagemaker/hunkim-transformer\n"
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='EsperBERTo', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.5.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    #train_instance_type='local',\n",
    "                    source_dir='code',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'tied': True\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\u001b[0m\n\u001b[34mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\u001b[0m\n\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n\u001b[34m2020-06-10 14:57:04,927 sagemaker-containers INFO     Invoking user script\n\u001b[0m\n\u001b[34mTraining Env:\n\u001b[0m\n\u001b[34m{\n    \"additional_framework_parameters\": {},\n    \"channel_input_dirs\": {\n        \"training\": \"/opt/ml/input/data/training\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {\n        \"tied\": true,\n        \"epochs\": 1\n    },\n    \"input_config_dir\": \"/opt/ml/input/config\",\n    \"input_data_config\": {\n        \"training\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"/opt/ml/input\",\n    \"is_master\": true,\n    \"job_name\": \"pytorch-training-2020-06-10-14-44-46-074\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"/opt/ml/model\",\n    \"module_dir\": \"s3://sagemaker-us-west-2-294038372338/pytorch-training-2020-06-10-14-44-46-074/source/sourcedir.tar.gz\",\n    \"module_name\": \"train\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 8,\n    \"num_gpus\": 1,\n    \"output_data_dir\": \"/opt/ml/output/data\",\n    \"output_dir\": \"/opt/ml/output\",\n    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"train.py\"\u001b[0m\n\u001b[34m}\n\u001b[0m\n\u001b[34mEnvironment variables:\n\u001b[0m\n\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n\u001b[34mSM_MODULE_NAME=train\u001b[0m\n\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n\u001b[34mSM_NUM_CPUS=8\u001b[0m\n\u001b[34mSM_NUM_GPUS=1\u001b[0m\n\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-294038372338/pytorch-training-2020-06-10-14-44-46-074/source/sourcedir.tar.gz\u001b[0m\n\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-10-14-44-46-074\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-294038372338/pytorch-training-2020-06-10-14-44-46-074/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n\u001b[34mSM_HP_TIED=true\u001b[0m\n\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n\u001b[0m\n\u001b[34mInvoking script with the following command:\n\u001b[0m\n\u001b[34m/opt/conda/bin/python train.py --epochs 1 --tied True\n\n\u001b[0m\n\u001b[34mNamespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=1, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\u001b[0m\n\u001b[34mEncoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\u001b[0m\n\u001b[34m['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']\u001b[0m\n\u001b[34mTraining...\u001b[0m\n\u001b[34m[2020-06-10 14:59:41.707 algo-1:72 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n\u001b[34m[2020-06-10 14:59:41.708 algo-1:72 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n\u001b[34m[2020-06-10 14:59:41.708 algo-1:72 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n\u001b[34m[2020-06-10 14:59:41.731 algo-1:72 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n\u001b[34m[2020-06-10 14:59:41.731 algo-1:72 INFO hook.py:422] Hook is writing from the hook with pid: 72\n\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.454 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.454 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.454 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.455 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.459 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.459 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.459 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.461 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.461 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.461 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.462 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.463 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.463 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.463 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.465 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.465 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.465 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.465 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.466 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.467 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.467 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.468 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.468 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.468 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.469 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.470 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.470 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.470 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.472 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.472 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.472 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.472 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.473 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.473 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.473 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.475 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.475 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.475 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.476 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.477 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.477 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34m[2020-06-10 14:59:42.477 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34m{\"loss\": 3.726819982290268, \"learning_rate\": 1.9097651421508038e-05, \"epoch\": 0.6180469715698393, \"step\": 500}\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.421 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.422 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.422 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.422 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.423 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.423 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.423 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.424 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.424 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.424 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.425 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.426 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.426 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.426 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.427 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.427 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.427 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.428 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.429 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.429 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.429 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.430 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.430 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.430 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.431 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.431 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.431 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.431 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.433 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.433 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.433 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.433 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.434 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.434 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.434 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.435 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.435 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.436 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.436 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.437 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.437 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34m[2020-06-10 15:00:16.437 algo-1:72 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n\u001b[34mSaving model\u001b[0m\n\u001b[34m[2020-06-10 15:00:37.299 algo-1:72 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n\u001b[34m2020-06-10 15:00:37,880 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n\n2020-06-10 15:00:39 Uploading - Uploading generated training model\n2020-06-10 15:01:31 Completed - Training job completed\nTraining seconds: 675\nBillable seconds: 675\nCPU times: user 1min 43s, sys: 46.3 s, total: 2min 29s\nWall time: 17min\n"
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s3://sagemaker-us-west-2-294038372338/pytorch-training-2020-06-10-14-44-46-074/output/model.tar.gz\n"
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(trained_model_location)\n",
    "# s3://sagemaker-us-west-2-294038372338/pytorch-training-2020-06-10-11-15-27-506/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.5.0',\n",
    "                     entry_point='infer.py',\n",
    "                     source_dir='code',\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n----------------!pytorch-inference-2020-06-10-15-05-28-352\n"
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "# Get the end point\n",
    "endpoint = predictor.endpoint\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'sequence': '<s> La sunomal.</s>', 'score': 0.016554249450564384, 'token': 1555}, {'sequence': '<s> La sunoa.</s>', 'score': 0.011879165656864643, 'token': 69}, {'sequence': '<s> La sunoÅĿ.</s>', 'score': 0.009716324508190155, 'token': 16886}, {'sequence': '<s> La sunore.</s>', 'score': 0.008137395605444908, 'token': 286}, {'sequence': '<s> La sunos.</s>', 'score': 0.007335992064327002, 'token': 87}]\n"
    }
   ],
   "source": [
    "#FIXME: This works, but the predictor using endpoint does not work\n",
    "input = {\n",
    "    'text': \"La suno <mask>.\"\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'[{\"sequence\": \"<s> La sunomal.</s>\", \"score\": 0.016554249450564384, \"token\": 1555}, {\"sequence\": \"<s> La sunoa.</s>\", \"score\": 0.011879165656864643, \"token\": 69}, {\"sequence\": \"<s> La suno\\\\u00c5\\\\u013f.</s>\", \"score\": 0.009716324508190155, \"token\": 16886}, {\"sequence\": \"<s> La sunore.</s>\", \"score\": 0.008137395605444908, \"token\": 286}, {\"sequence\": \"<s> La sunos.</s>\", \"score\": 0.007335992064327002, \"token\": 87}]'\n"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "input = {\n",
    "    'text': \"La suno <mask>.\"\n",
    "}\n",
    "\n",
    "content_type = \"application/json \"                                     \n",
    "accept = \"application/json\"                                       \n",
    "payload = json.dumps(input)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint, \n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "print(response['Body'].read())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitvenvvenvfbc6d215d2384daca7c44542d667cdba",
   "display_name": "Python 3.7.3 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}